# Day 2
This was a pretty fun day. About five hours spend in a combination of reading, watching, and experimenting. I covered OOP, kwargs/args, nested loops, error handling, string manipulation, and more advanced ifelse chains. Lots of mini projects to reinforce topics. Especially interesting were classes, as I'd seen them before but hadnt actually written. They seem super useful and clean but I'd like to know a bit more about scope within them and what conventions are. The projects were: a calculator, a shopping cart, and finally a racing car simulator built around classes. Looking forward to numpy tomorrow.
# Day 3
This was a more specialized day. I continued learning about classes and inheritance, and also multiple inheritance. I also started working on numpy fundamentals, such as the basic operations with matrices, the indexing operations, and how to create them from files.
# Day 4
I dove into numpy fundamentals for the first part of the day. I learned many ways to manipulate arrays, both array-wise and by index. I learned about and applied the linear algebra that numpy enables. To cap the day off, I coded a totally from-scratch 2 layer neural network that predicts XOR gate outputs from inputs. The model, after 1000 iterations, predicted correctly with > 98% accuracy.
# Day 5 
Today was the first day I felt like I was truly working on ML, surface level though it may be for now. I refined the toy network with three layers, added hyperparameters like learning rate and early stopping, and began to learn pytorch for tomorrow. The model is even more accurate now, being about 98.5% confident in its predictions for the XOR gate. I also did about 30 minutes of python basics, learning about the super method and inhertiance.
# Day 6.
First, I added batching into the toy network. I gave the model batches of 2 input/output pairs at a time. After that, I moved to Pytorch. After building a network from scratch, finally using Pytorch felt like a cheat code. I rebuilt the exact same network in torch, then optimized a few things such as switching the final layer to sigmoid from ReLU. This, plus an order of magnitute more epochs and a lower learning rate, allowed the network to hit over 99.9% confidence. Tomorrow